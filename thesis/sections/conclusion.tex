\chapter{Conclusion} \label{chap:conclusion} % 1 pages

% Summary of problem and contribution
In this work, we have proposed modifications to the generative adversarial network for video (VGAN) architecture, to allow it to make next-frame predictions based on a fixed number of input frames. It does so by using a fully decoupled two-stream generator network, which enables it to learn motion and static scene dynamics independently, improving its generalization capabilities across contexts. Training of this model is done in unsupervised and adversarial fashion, which is an advantage when utilizing this model for video anomaly detection. Large amounts of unlabeled but mostly normal data is often readily available and allows us to train a model that reflects the normal state of to be analyzed video streams. When embedded into the IFTM framework, the model forecasts future values of the stream in real-time, using a sliding window of the past. High differences to the actual monitored video frames are classified as anomalies. Due to the use of IFTM, such a system does not require any domain expertise on how to set the boundary between normal and anomalous video frames, since the threshold model is also trained in unsupervised mode and does not need any additional hyperparameters to be tuned. We apply and evaluate this proposed video anomaly detection system for the use case of CCTV video data. With over 240 hours of unlabeled video data, we challenge our model to learn a variety of object patterns and contexts in which they can occur normally. 

% Summary of results
Experimental evaluation using hold-out validation and a separate labeled evaluation data set show promising results of our proposed model. While adversarial training is often unstable, we achieve a lasting equilibrium in which our next-frame prediction model continuously improves in quality. It is able to identify the context of a video sample and make predictions based on that and not on the currently occurring motion patterns alone. For the use case of anomaly detection, our threshold model has sometimes difficulties to find a satisfying threshold value that is not too sensitive, causing a high false positive rate for some of our configurations. This is the case due to the iterative training of the GAN on the training set, which decreases the prediction error on that data set faster than on unknown videos. However we are able to counteract this phenomenon by putting parts of the training data on the side for the training phase of the forecasting model, before that split is later used to train the threshold. This results in an overall higher threshold value that more accurately reflects the boundary between the two classes, outperforming all our unmodified configurations. In general our models' predictions have a stable error for normal events, with rapidly changing spikes for anomalous frames. On the other hand, the system struggles especially with anomalous static object patterns, because the underlying video forecasting model is able to generalize well enough to make good predictions for them. In addition there are rare instances of normal frames that have very high errors before the error returns back to normal, which can be traced back to quick unforeseeable movements in the video.

% Outlook
In future work, we will explore ways to address the latter problem. This includes smoothing models, which will cause the model to omit sudden changes in the error. Though this comes at a price in regards to the time until an actual anomaly will have been detected. In addition, different ways to rate the quality of a prediction compared to the actual future need to be assessed. It could also lead to better detection results, if one were to decouple the computation of such an error for the appearance and motion patterns, and weight these two differently for classification. Finally there is still the challenge of detecting anomalous frames, that seem normal from a short-term perspective, but require a variable knowledge of the past to be identified. Capturing more of the temporal coherences of a video stream can be accomplished using RNN components, that dynamically learn to forget past observations, depending on their relevance.