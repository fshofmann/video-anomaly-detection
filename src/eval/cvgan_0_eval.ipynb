{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Fabian Hofmann\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# (0) Evaluation of C-VGAN\n",
    "\n",
    "In this notebook, the evaluation of the original infrastructure C-VGAN is done (N out 1 frames are generated). For our proposed approach, check the other C-VGAN evaluation scripts. Note C-VGAN in its original form is exclusively about video generation (and reconstruction for the input frames), so this evaluation excludes any anomaly detection evaluation - this is done in the respective IFTM evaluation with the best models built and evaluated here. The original conditional video generation is evaluated to inspect the width of the bottleneck and how it affects the generator's ability to reconstruct past frames.\n",
    "\n",
    "The models evaluated here are built in the same way as in the first scripts and they are evaluated based on their hyperparameters. However, due to the time required to train a model fully (>48 hours), a classical grid search or other optimization of the hyperparameters is difficult if not unfeasible without the necessary computation resources. Therefore we base most of our parameters on the the original VGAN paper by Vondrick et al. [1](http://www.cs.columbia.edu/~vondrick/tinyvideo/), and hard-code the different parameters we want to evaluate for the model directly (there is no point in iterating over the different permutations of the hyperparameters, if running the entire evaluation would take weeks if not months). The notebook is also designed to be shut down and continued at a later point in time using checkpoints due to the long running time.\n",
    "\n",
    "##### Sources\n",
    "- [Our implementation of C-VGAN](https://gitlab.tubit.tu-berlin.de/sulandir/Thesis/blob/master/src/models/vgan_conditional.ipynb)\n",
    "- [The preset VGAN models and their streams](https://gitlab.tubit.tu-berlin.de/sulandir/Thesis/blob/master/src/models/vgan.py)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from src.io_.batch_generator import UniformBatchGenerator, BatchLoader\n",
    "from src.io_.gif_writer import write_gif_grid\n",
    "from src.models.vgan import *"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_input_path = \"../../data/upCam/preprocessed_128_64/\"\n",
    "base_output_path = \"../../output/evaluation/vgan/vgan_conditional/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Init batch generator for training/validation/testing\n",
    "\n",
    "As in the original C-VGAN with 2d input notebook [1](https://gitlab.tubit.tu-berlin.de/sulandir/Thesis/blob/master/src/models/vgan_conditional.ipynb) the generator gets the first frame as input and it has to generate an 7 frames for it (total of 8). The discriminator gets the actual ~1.5 second (8 frames) video and learns to distinguish the actual from the fake/generated video clips.\n",
    "\n",
    "The [dataset](https://gitlab.tubit.tu-berlin.de/sulandir/thesis_data) consists of 10 days (days \"00-04\" and \"05-09\" collected in sets). For evaluation, the dataset is split -- 60% is used for training, 20% for validation during training and 20% for testing.\n",
    "\n",
    "Note that the batch size for validation and testing set is meaningless -- it merely serves as a (minor) speedup and has no influence on the actual losses."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "batch_sizes = [4, 8, 16, 32, 64, 128, 256]\n",
    "BATCH_SIZE = batch_sizes[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "days_training = [\"00\", \"01\", \"02\", \"05\", \"06\", \"07\"]\n",
    "day_paths_training = [base_input_path + day + \"/\" for day in days_training]\n",
    "training_generator = UniformBatchGenerator(day_paths=day_paths_training, batch_size=BATCH_SIZE, sample_size=8, subsample_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "days_validation = [\"03\", \"08\"]\n",
    "day_paths_validation = [base_input_path + day + \"/\" for day in days_validation]\n",
    "validation_generator = UniformBatchGenerator(day_paths=day_paths_validation, batch_size=256, sample_size=8, subsample_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "days_testing = [\"04\", \"09\"]\n",
    "day_paths_testing = [base_input_path + day + \"/\" for day in days_testing]\n",
    "testing_generator = UniformBatchGenerator(day_paths=day_paths_testing, batch_size=256, sample_size=8, subsample_size=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## C-VGAN Models\n",
    "\n",
    "C-VGAN models are built and trained just like in [vgan_conditional.ipynb](https://gitlab.tubit.tu-berlin.de/sulandir/Thesis/blob/master/src/models/vgan_conditional.ipynb), with the exception of the bottleneck; using a filter number of `1024` at the bottleneck instead of `512`. This is in parallel to the original code of the proposed future frame prediction model, that uses a widened bottleneck instead of the original model."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_generator_model() -> keras.Model:\n",
    "    inputs: tf.Tensor = keras.Input(shape=(1, 64, 128, 3))\n",
    "\n",
    "    # Reshape into 2D\n",
    "    x: tf.Tensor = layers.Reshape((64, 128, 3))(inputs)\n",
    "    assert x.get_shape().as_list() == [None, 64, 128, 3]\n",
    "\n",
    "    e: tf.Tensor = make_encoder_stream(x)\n",
    "\n",
    "    # Adapt encoded input to background stream format and create stream\n",
    "    e_3d: tf.Tensor = layers.Reshape((1, 2, 4, 1024))(e)\n",
    "    e_3d =  layers.Conv3DTranspose(filters=1024, kernel_size=3, strides=(2, 1, 1),\n",
    "                                   padding=\"same\", use_bias=False, kernel_initializer=\"he_normal\")(e_3d)\n",
    "    assert e_3d.shape.as_list() == [None, 2, 2, 4, 1024]\n",
    "    e_3d = layers.BatchNormalization()(e_3d)\n",
    "    e_3d = layers.ReLU()(e_3d)\n",
    "    f, m = make_conditional_foreground_stream(e_3d)\n",
    "\n",
    "    b = make_conditional_background_stream(e)\n",
    "\n",
    "    outputs = make_generator_stream_combiner(f, m, b)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=\"c_vgan_generator\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "c_vgan_generator = make_generator_model()\n",
    "c_vgan_generator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keras.utils.plot_model(c_vgan_generator, base_output_path + \"img/c_vgan_generator.png\", show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The discriminator usually overpowers the generator (failure mode) because of the properties of the data (it is too easy to distinguish real from fake ones. Therefore, two ways to impair the discriminator can be used; a reduction/increase in the number of filters on each convolutional layers (default `32`, multiplied by 2,4 , etc. for later layers respectively) and dropout layers that are applied to each convolutional layer (default rate `0`)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filters = [8, 16, 32, 64]\n",
    "FILTER = filters[1]\n",
    "\n",
    "dropout_rates = [0., 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "DROPOUT_RATE = dropout_rates[0]\n",
    "\n",
    "c_vgan_discriminator = make_discriminator_model(FILTER, DROPOUT_RATE)\n",
    "c_vgan_discriminator.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "keras.utils.plot_model(c_vgan_discriminator, base_output_path + \"img/c_vgan_discriminator.png\", show_shapes=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss and optimizers init"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "mean_abs_err = tf.keras.losses.MeanAbsoluteError()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
    "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
    "    return real_loss + fake_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lambda for is kept to its original value from the paper, because it can only be evaluated through IFTM and not through validator (and test) losses, because it influences the quality of the 8th frame prediction and weights the input to the pure generational output (a smaller lambda increases the degrees of freedom for the generator, while keeping the scenes for the discriminator realistic, but in return the scenes will less resemble the conditional input (from which to interpolate the 8th frame)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "lambdas = [5, 8, 10, 14, 20]\n",
    "LAMBDA = lambdas[2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generator_loss(fake_output, input_frames, generated_videos):\n",
    "    fake_loss = cross_entropy(tf.ones_like(fake_output), fake_output)\n",
    "    reconstruction_loss = mean_abs_err(input_frames, generated_videos[:,:1]) * LAMBDA\n",
    "    return fake_loss + reconstruction_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "When adjusting `BATCH_SIZE`, one also has to change the original learning rate by a factor [\\[1\\]](https://arxiv.org/abs/1404.5997). The momentum term (`beta_1`) is kept the same however. Note that because ADAM is used as an optimizer, the learning rate is adapted either way."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "learning_rates_mult = [0.5, 1, 2, 4]\n",
    "LEARNING_RATE = math.sqrt(learning_rates_mult[1]) * 2e-4"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up separate outputs for configs\n",
    "\n",
    "Logs, graphs, and checkpoints for different parameter permutations are stored in separate files and directories to make the handling of the different configs easier later."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "OUTPUT_PREFIX = \"cvgan_0_dfilter-{}_ddropout-{}_lambda-{}_batchsize-{}_learningrate-{}\".format(FILTER, DROPOUT_RATE, LAMBDA, BATCH_SIZE, LEARNING_RATE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handle checkpoints"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "checkpoint_dir = base_output_path + \"training_checkpoints/\" + OUTPUT_PREFIX + \"/\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=c_vgan_generator,\n",
    "                                 discriminator=c_vgan_discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the latest checkpoint (if available). This is crucial in case one wants to run longer training sessions that are unfeasible as a continuous run."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint.restore(latest_checkpoint)\n",
    "if latest_checkpoint:\n",
    "    print(\"Restored from {}\".format(latest_checkpoint))\n",
    "else:\n",
    "    print(\"Initializing from scratch.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handle Logfiles\n",
    "\n",
    "For the (csv) log files, during training the results are appended to the existing ones - this further allows the continuous evaluation of a parameter setting across a multitude of epochs."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "LOG_FILE = base_output_path + OUTPUT_PREFIX + \".csv\"\n",
    "\n",
    "try:\n",
    "    file = open(LOG_FILE, 'r')\n",
    "    print(\"Existing log file found, evaluation results will be appended to it.\")\n",
    "except IOError:\n",
    "    file = open(LOG_FILE, 'w')\n",
    "    print(\"Initializing new log file.\")\n",
    "    file.write(\"g_train_loss;d_train_loss;g_val_loss;d_val_loss\\n\")\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the training loop\n",
    "\n",
    "Nothing is changed regarding the kind of training and optimization of the gradients, however a `test_step` was added to evaluate the validator loss between epoch (and the test loss after full training)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    input_frames, real_videos = batch\n",
    "\n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        generated_videos = c_vgan_generator(input_frames, training=True)\n",
    "\n",
    "        real_output = c_vgan_discriminator(real_videos, training=True)\n",
    "        fake_output = c_vgan_discriminator(generated_videos, training=True)\n",
    "\n",
    "        gen_loss = generator_loss(fake_output, input_frames, generated_videos)\n",
    "        disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, c_vgan_generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, c_vgan_discriminator.trainable_variables)\n",
    "\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, c_vgan_generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, c_vgan_discriminator.trainable_variables))\n",
    "\n",
    "    return gen_loss, disc_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def test_step(batch):\n",
    "    input_frames, real_videos = batch\n",
    "\n",
    "    generated_videos = c_vgan_generator(input_frames, training=False)\n",
    "\n",
    "    real_output = c_vgan_discriminator(real_videos, training=False)\n",
    "    fake_output = c_vgan_discriminator(generated_videos, training=False)\n",
    "\n",
    "    gen_loss = generator_loss(fake_output, input_frames, generated_videos)\n",
    "    disc_loss = discriminator_loss(real_output, fake_output)\n",
    "\n",
    "    return gen_loss, disc_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Every training iteration, the overall loss for generator and discriminator is computed (both training and validation) and then output along with the total time for training of the epoch. The function below is used for both training, validation, and testing."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def exec_epoch(batch_generator: UniformBatchGenerator, step_fn: Callable):\n",
    "    g_loss = keras.metrics.Mean()\n",
    "    d_loss = keras.metrics.Mean()\n",
    "\n",
    "    batch_loader = BatchLoader(batch_generator, 2048 // batch_generator.batch_size, no_workers=4)\n",
    "    for _ in range(len(batch_loader)):\n",
    "        batch = batch_loader.get_batch()\n",
    "        g_batch_loss, d_batch_loss = step_fn(batch)\n",
    "        g_loss.update_state(g_batch_loss)\n",
    "        d_loss.update_state(d_batch_loss)\n",
    "    batch_loader.shutdown_workers()\n",
    "\n",
    "    return g_loss.result().numpy(), d_loss.result().numpy()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train(epochs, start_epoch=0):\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "        # train\n",
    "        start = time.time()\n",
    "        g_train_loss, d_train_loss = exec_epoch(training_generator, train_step)\n",
    "        stop = time.time()\n",
    "\n",
    "        # validate\n",
    "        g_val_loss, d_val_loss = exec_epoch(validation_generator, test_step)\n",
    "\n",
    "        # Create checkpoint every 10 epochs\n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)\n",
    "\n",
    "        # Generate some example videos for visual help\n",
    "        generate_and_save_videos(c_vgan_generator, epoch + 1, examples_input)\n",
    "\n",
    "        # Append metrics to log file\n",
    "        log = open(LOG_FILE, 'a')\n",
    "        # noinspection PyTypeChecker\n",
    "        np.savetxt(log, [[g_train_loss, d_train_loss, g_val_loss, d_val_loss]], fmt=\"%.8f\", delimiter=\";\")\n",
    "        log.close()\n",
    "\n",
    "        print(\"Epoch {}: Training Time - {:.2f} sec, g_train_loss - {:.8f}, d_train_loss - {:.8f}, g_val_loss - {:.8f}, d_val_loss - {:.8f},\".format(epoch + 1, stop - start, g_train_loss, d_train_loss, g_val_loss, d_val_loss))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Generate and save videos\n",
    "\n",
    "For monitor the training progress, we generate 16 test videos each epoch as a sanity check whether the generator still produces a variety of videos (this is done beside the train and validation loss metrics); video inputs are taken from the first batch of the test batch."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "examples_input = testing_generator[0][0][::16]\n",
    "examples_output = testing_generator[0][1][::16]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "anim_file_dir = base_output_path + \"anim/\" + OUTPUT_PREFIX + \"/\"\n",
    "\n",
    "if not os.path.exists(anim_file_dir):\n",
    "    os.makedirs(anim_file_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_anim_grid = base_output_path + \"anim/\" + \"test_anim.gif\"\n",
    "write_gif_grid(test_anim_grid, examples_output, fps=5, dpi=600)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_and_save_videos(model, epoch, test_input):\n",
    "    predictions = model(test_input, training=False).numpy()\n",
    "\n",
    "    anim_grid = anim_file_dir + \"anim_at_epoch_{:04d}.gif\".format(epoch)\n",
    "    write_gif_grid(anim_grid, predictions, fps=5, dpi=600)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the model\n",
    "\n",
    "If one continues from a checkpoint, `start_epoch` should be set to the checkpoint's epoch (`=0` for no checkpoint). Will only influence logs inside the notebook and file names for any files being generated; loss logs will be appended as normal."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "train(epochs=50, start_epoch=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the model\n",
    "\n",
    "Run over the entire test set and compute both generator and discriminator test loss."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(exec_epoch(testing_generator, test_step))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-7d957dfd",
   "language": "python",
   "display_name": "PyCharm (Thesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}