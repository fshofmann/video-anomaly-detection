{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Copyright 2020 Fabian Hofmann\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluation of IFTM\n",
    "\n",
    "The evaluation of IFTM using video generation models strictly depends on the generators -- IFTM on its own past the used reconstructor/predictor does not use any additional parameters when using cumulative aggregation. Therefore, in this notebook, the models that were trained in the C-VGAN (with video input) evaluation notebooks are simply loaded and passed to IFTM as its forecasting model.\n",
    "\n",
    "##### Sources\n",
    "- [Implementation of IFTM](https://gitlab.tubit.tu-berlin.de/sulandir/Thesis/blob/master/src/models/iftm.py)\n",
    "- [Implementation and evaluation of C-VGAN](https://gitlab.tubit.tu-berlin.de/sulandir/Thesis/blob/master/src/eval/cvgan_2_eval.ipynb)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "'2.5.0-dev20201111'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "tf.__version__"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default GPU Device: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "if tf.test.gpu_device_name():\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Please install GPU version of TF\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import csv\n",
    "import cv2\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "\n",
    "from src.io_.batch_generator import UniformBatchGenerator, SlidingWindowBatchGenerator, BatchLoader\n",
    "from eval.anomaly_detection_evaluation import AnomalyDetectionEvaluation\n",
    "from src.models.vgan import *\n",
    "from src.models.iftm import IFTM"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "base_input_path = \"../../data/upCam/preprocessed_128_64/\"\n",
    "base_input_path_model = \"../../output/evaluation/vgan/vgan_conditional_3d_2/\"\n",
    "base_input_path_eval = \"../../data/eval/\"\n",
    "base_output_path = \"../../output/evaluation/iftm/\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Init batch generator for training/testing\n",
    "\n",
    "The data used is the same as in the C-VGAN evaluation notebook, but the original validation and testing sets are merged -- they simply serve as a sanity check whether the model does not label mostly normal data as anomalies. Thus they do not contribute to any of the evaluation metrics. The training set needs to be used to train the threshold model of IFTM.\n",
    "\n",
    "The actual evaluation dataset is separate and can be found in the same [data repository](https://gitlab.tubit.tu-berlin.de/sulandir/thesis_data/eval). Each video is one hour long and consists of labeled normal and anomalous frames. The labels for these are in a separate `.csv` file (one file per video)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "days_training = [\"00\", \"01\", \"02\", \"05\", \"06\", \"07\"]\n",
    "day_paths_training = [base_input_path + day + \"/\" for day in days_training]\n",
    "training_generator = UniformBatchGenerator(day_paths=day_paths_training, batch_size=256, sample_size=8, subsample_size=7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "days_testing = [\"03\", \"04\", \"08\", \"09\"]\n",
    "day_paths_testing = [base_input_path + day + \"/\" for day in days_testing]\n",
    "testing_generator = UniformBatchGenerator(day_paths=day_paths_testing, batch_size=256, sample_size=8, subsample_size=7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "eval_files = [\"00\"]\n",
    "eval_paths = [base_input_path_eval + \"preprocessed_128_64/\" + file + \".mp4\" for file in eval_files]\n",
    "eval_generator = SlidingWindowBatchGenerator(file_paths=eval_paths, batch_size=256, sample_size=8, subsample_size=7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## C-VGAN Models\n",
    "\n",
    "The models here are simply defined (they are the same as in [cvgan_2_eval.ipynb](https://gitlab.tubit.tu-berlin.de/sulandir/Thesis/blob/master/src/eval/cvgan_2_eval.ipynb)), so we can utilize the tensorflow checkpointing API and not resort to fully exported (and then loaded) models that can no longer be properly accessed.\n",
    "\n",
    "Training of these is not supported in this notebook however."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model init"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "def make_generator_model() -> keras.Model:\n",
    "    inputs: tf.Tensor = keras.Input(shape=(7, 64, 128, 3))\n",
    "\n",
    "    e_3d = make_encoder_foreground_stream(inputs)\n",
    "    f, m = make_conditional_foreground_stream(e_3d)\n",
    "\n",
    "    e_2d = make_encoder_background_stream(inputs)\n",
    "    b = make_conditional_background_stream(e_2d)\n",
    "\n",
    "    outputs = make_generator_stream_combiner(f, m, b)\n",
    "\n",
    "    return keras.Model(inputs=inputs, outputs=outputs, name=\"c_vgan_generator\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "c_vgan_generator = make_generator_model()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "filters = [8, 16, 32, 64]\n",
    "FILTER = filters[1]\n",
    "\n",
    "dropout_rates = [0., 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "DROPOUT_RATE = dropout_rates[3]\n",
    "\n",
    "c_vgan_discriminator = make_discriminator_model(FILTER, DROPOUT_RATE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Optimizers init\n",
    "\n",
    "Because the original checkpoints do store the optimizers, they are simply initialized for completeness sake although they are not used in any way."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "learning_rates_mult = [0.5, 1, 2, 4]\n",
    "LEARNING_RATE = math.sqrt(learning_rates_mult[1]) * 2e-4\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(learning_rate=LEARNING_RATE, beta_1=0.5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set up separate input and outputs for configs\n",
    "\n",
    "Because the models were build using different parameter permutations and their checkpoints were stored in different directories (during c-vgan evaluation), this notebook can only evaluate one parameter permutation at a time - loading multiple models into a single tensorflow session causes memory leaks (and requires a tf session clear). Therefore the config that one wants to use with IFTM has to be explicitly stated so the (latest) checkpoint of that config will be loaded.\n",
    "\n",
    "Any hyper-parameters that were set in training that are not already set have to be defined for that."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "lambdas = [5, 8, 10, 14, 20]\n",
    "LAMBDA = lambdas[3]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "batch_sizes = [4, 8, 16, 32, 64, 128, 256]\n",
    "BATCH_SIZE = batch_sizes[4]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "In addition, to allow the evaluation of earlier versions of the model with less epochs used to train it (see checkpoint loading), there is an additional param passed to the prefix:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "EPOCHS = 50"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "OUTPUT_PREFIX_CKPT = \"cvgan_2_dfilter-{}_ddropout-{}_lambda-{}_batchsize-{}_learningrate-{}\".format(FILTER, DROPOUT_RATE, LAMBDA, BATCH_SIZE, LEARNING_RATE)\n",
    "\n",
    "OUTPUT_PREFIX = \"cvgan_2_dfilter-{}_ddropout-{}_lambda-{}_batchsize-{}_learningrate-{}_epochs-{:04d}\".format(FILTER, DROPOUT_RATE, LAMBDA, BATCH_SIZE, LEARNING_RATE, EPOCHS)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load latest checkpoint"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "checkpoint_dir = base_input_path_model + \"training_checkpoints/\" + OUTPUT_PREFIX_CKPT + \"/\"\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=c_vgan_generator,\n",
    "                                 discriminator=c_vgan_discriminator)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can either load the latest checkpoint of a trained model which is desired, or one can manually override the parameter of `checkpoint.restore()` to load a checkpoint of an earlier epoch; this is akin to \"early stopping\", to avoid overfitting if the model's quality worsened over the course of training."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restored from ../../output/evaluation/vgan/vgan_conditional_3d_2/training_checkpoints/cvgan_2_dfilter-16_ddropout-0.3_lambda-14_batchsize-64_learningrate-0.0002/ckpt-5\n"
     ]
    }
   ],
   "source": [
    "# n_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "n_checkpoint = checkpoint_prefix + \"-{}\".format(EPOCHS // 10)\n",
    "checkpoint.restore(n_checkpoint)\n",
    "if n_checkpoint:\n",
    "    print(\"Restored from {}\".format(n_checkpoint))\n",
    "else:\n",
    "    raise FileNotFoundError(\"Checkpoint of model not found!\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Make predictions on evaluation videos\n",
    "\n",
    "Before using IFTM for anomaly detection, the evaluation video samples are fed to the video generator to make next frame (`n+1`) predictions over the entire dataset. Both generated and actual videos are saved to disk. This is not directly used in the evaluation but serves as a visual guide for late use.\n",
    "\n",
    "Note that because the evaluation data must not be shuffled (the time component is crucial for evaluation purposes), a `BatchLoader` must not be used and the data has to be read sequentially, batch by batch\n",
    "\n",
    "***DISCLAIMER:*** Will cost a ton of memory (and time) due to a high number of redundant frames (x8), that will cost high amounts of storage space without much gain of information."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# anim_file_dir_real = base_output_path + \"anim/\" + \"real/\"\n",
    "# anim_file_dir_gen = base_output_path + \"anim/\" + OUTPUT_PREFIX + \"/\"\n",
    "#\n",
    "# if not os.path.exists(anim_file_dir_real):\n",
    "#     os.makedirs(anim_file_dir_real)\n",
    "#\n",
    "# if not os.path.exists(anim_file_dir_gen):\n",
    "#     os.makedirs(anim_file_dir_gen)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# start_gen_testing = time.time()\n",
    "#\n",
    "# count = 0\n",
    "# for k in range(len(eval_generator)):\n",
    "#     input_videos, real_videos = eval_generator[k]\n",
    "#     generated_videos = c_vgan_generator(input_videos, training=False).numpy()\n",
    "#     for i in range(len(generated_videos)):\n",
    "#         write_gif(anim_file_dir_real + \"{:06d}.gif\".format(count), real_videos[i])\n",
    "#         write_gif(anim_file_dir_gen + \"{:06d}.gif\".format(count), generated_videos[i])\n",
    "#         count += 1\n",
    "#\n",
    "# print('Time for generation of evaluation videos is {:.4f} sec'.format(time.time() - start_gen_testing))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## IFTM\n",
    "\n",
    "This section is a copy to our initial [iftm notebook](https://gitlab.tubit.tu-berlin.de/sulandir/Thesis/blob/master/src/models/iftm.ipynb)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the error function\n",
    "\n",
    "Only the last frame of each sequence matters - only forecasting, not reconstruction of past frames is relevant to the identity function result."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "reconstruction_err = tf.keras.losses.MeanAbsoluteError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "# reconstruction_err = tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "def error_function(predicted: tf.Tensor, actual: tf.Tensor) -> tf.Tensor:\n",
    "    predicted = tf.reshape(predicted[:,-1], shape=[predicted.shape[0],-1])\n",
    "    actual = tf.reshape(actual[:,-1], shape=[actual.shape[0],-1])\n",
    "    return reconstruction_err(predicted, actual)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Init and train IFTM"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "iftm = IFTM(c_vgan_generator, error_function)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The training data that was used to train the C-VGAN model is now rerun on the model to compute the training (forecasting) error to train the threshold model of IFTM as well."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for TM training is 469.2395 sec\n"
     ]
    }
   ],
   "source": [
    "start_tm_training = time.time()\n",
    "iftm.train_threshold(training_generator, max_queue_size=8, no_workers=4)\n",
    "\n",
    "print('Time for TM training is {:.4f} sec'.format(time.time() - start_tm_training))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Print out the threshold:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09476271\n"
     ]
    }
   ],
   "source": [
    "print(iftm.threshold)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make test predictions\n",
    "\n",
    "As said in the beginning the test set is the union of testing and validation set during the training of the C-VGAN model. This is not an actual evaluation but simply a check how the model performs on data that should be considered (mostly) normal."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for IFTM testing is 285.6981 sec\n"
     ]
    }
   ],
   "source": [
    "start_iftm_testing = time.time()\n",
    "\n",
    "testing_batches = BatchLoader(testing_generator, max_queue_size=8, no_workers=4)\n",
    "predictions = []\n",
    "for _ in range(len(testing_batches)):\n",
    "    b_x, b_y = testing_batches.get_batch()\n",
    "    _, p = iftm.predict(b_x, b_y)\n",
    "    predictions.append(p)\n",
    "testing_batches.shutdown_workers()\n",
    "\n",
    "print('Time for IFTM testing is {:.4f} sec'.format(time.time() - start_iftm_testing))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Count and output the number of \"false\" (0, normal) and \"true\" (1, anomaly) predictions."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[178281  36759]\n"
     ]
    }
   ],
   "source": [
    "predictions = np.array(predictions).flatten()\n",
    "p_count = np.bincount(predictions)\n",
    "\n",
    "print(p_count)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation\n",
    "\n",
    "For the evaluation, a separate [evaluation dataset](https://gitlab.tubit.tu-berlin.de/sulandir/thesis_data/eval) was created. It consists of one or more one hour long videos and a separate `.csv` file with the true labels (`0,1`). Therefore, one can make predictions using the IFTM model and then compute the evaluation metrics using the true labels."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Make predictions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for IFTM eval is 29.3816 sec\n"
     ]
    }
   ],
   "source": [
    "start_iftm_testing = time.time()\n",
    "\n",
    "if_results = []\n",
    "predictions = []\n",
    "for k in range(len(eval_generator)):\n",
    "    b_x, b_y = eval_generator[k]\n",
    "    if_, p = iftm.predict(b_x, b_y)\n",
    "    if_results.append(if_)\n",
    "    predictions.append(p)\n",
    "\n",
    "if_results = np.array(if_results).flatten()\n",
    "predictions = np.array(predictions).flatten()\n",
    "\n",
    "print('Time for IFTM eval is {:.4f} sec'.format(time.time() - start_iftm_testing))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluate predictions\n",
    "\n",
    "The `AnomalyDetectionEvaluation` class loads the true labels and then computes the evaluation metrics for the given predicted labels of each individual sample."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "anomaly_detection_eval = AnomalyDetectionEvaluation(base_input_path_eval + \"labels/\" + \"00\" + \".csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For evaluation purposes for the first seven frames, to which there are no actual predictions, their predictions are explicitly set to 0 so the shape of predictions and actual values match. The same is done to the last frames for which no predictions were done as well, because they were not enough to create another batch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.7584966234916417, 'recall': 0.6014138118542687, 'true negative rate': 0.8275559805562196, 'precision': 0.6052535570959504, 'negative predictive value': 0.8252542911633821, 'false negative rate': 0.3985861881457314, 'false positive rate': 0.17244401944378038, 'f1 measure': 0.6033275752341122}\n"
     ]
    }
   ],
   "source": [
    "eval_results = anomaly_detection_eval.evaluate(np.concatenate((np.zeros(7), predictions, np.zeros(139))).astype(np.bool_))\n",
    "print(eval_results)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Store prediction and evaluation results\n",
    "\n",
    "The predictions of IFTM, both the forecasting errors for each sample and the anomaly detection result, is written to a CSV to be used later for graphs or diagrams. The evaluation metrics are also printed, but to a separate file."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Because the prediction error function can be adjusted, `OUTPUT_PREFIX` is altered as well to differentiate between different function usages."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "OUTPUT_PREFIX_EXT = OUTPUT_PREFIX + \"_errorfn-\" + reconstruction_err.name"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "RES_FILE = base_output_path + OUTPUT_PREFIX_EXT + \"_res.csv\"\n",
    "\n",
    "file = open(RES_FILE, 'w')\n",
    "# noinspection PyTypeChecker\n",
    "np.savetxt(file, np.array(list(zip(if_results, [iftm.threshold]*len(if_results), predictions))),\n",
    "           header=\"if_value;threshold;prediction\", fmt=\"%.8f\", delimiter=\";\")\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "EVAL_FILE = base_output_path + OUTPUT_PREFIX_EXT + \"_eval.csv\"\n",
    "\n",
    "file = open(EVAL_FILE, 'w')\n",
    "writer = csv.writer(file, delimiter=';')\n",
    "for row in eval_results.items():\n",
    "    writer.writerow(row)\n",
    "file.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### (Addendum) Visualize IF results\n",
    "\n",
    "Each of the IF results is in a range of `[0,1]`, only corresponds to the last frame of a sample and thus can be visualized in grayscale as a singular picture (for each sample). To do this, one needs to alter the error function from a per video (last frame) mean absolute error to a pixel-wise error (mean error over the RGB values) of the last frame for each video."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "def error_function(predicted: tf.Tensor, actual: tf.Tensor) -> tf.Tensor:\n",
    "    return reconstruction_err(predicted[:,-1], actual[:,-1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "img_dir_real =  base_output_path + \"img/\" + \"real/\"\n",
    "img_dir_pred = base_output_path + \"img/\" + OUTPUT_PREFIX + \"/predicted/\"\n",
    "img_dir_err = base_output_path + \"img/\" + OUTPUT_PREFIX + \"/error/\" + reconstruction_err.name + \"/\"\n",
    "\n",
    "if not os.path.exists(img_dir_real):\n",
    "    os.makedirs(img_dir_real)\n",
    "\n",
    "if not os.path.exists(img_dir_pred):\n",
    "    os.makedirs(img_dir_pred)\n",
    "\n",
    "if not os.path.exists(img_dir_err):\n",
    "    os.makedirs(img_dir_err)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for generation of forecasting frames is 125.9141 sec\n"
     ]
    }
   ],
   "source": [
    "start_gen_testing = time.time()\n",
    "\n",
    "count = 0\n",
    "for k in range(len(eval_generator)):\n",
    "    input_videos, real_videos = eval_generator[k]\n",
    "    generated_videos = c_vgan_generator(input_videos, training=False)\n",
    "    errors_per_sample = error_function(generated_videos, tf.convert_to_tensor(real_videos)).numpy()\n",
    "\n",
    "    # prepare frames for output\n",
    "    error_frames = errors_per_sample * 127.5 + 127.5\n",
    "    real_frames = (real_videos[:,-1] * 127.5 + 127.5).astype(\"uint8\")\n",
    "    predicted_frames = (generated_videos.numpy()[:,-1] * 127.5 + 127.5).astype(\"uint8\")\n",
    "\n",
    "    for i in range(len(errors_per_sample)):\n",
    "        # actual last frame\n",
    "        cv2.imwrite(img_dir_real + \"{:06d}.png\".format(count), real_frames[i])\n",
    "        # predicted last frame\n",
    "        cv2.imwrite(img_dir_pred + \"{:06d}.png\".format(count), predicted_frames[i])\n",
    "        # error heatmap of frame\n",
    "        plt.imsave(img_dir_err + \"/\" + \"{:06d}.png\".format(count), error_frames[i], cmap='gray_r')\n",
    "        count += 1\n",
    "\n",
    "print('Time for generation of forecasting frames is {:.4f} sec'.format(time.time() - start_gen_testing))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-7d957dfd",
   "language": "python",
   "display_name": "PyCharm (Thesis)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}